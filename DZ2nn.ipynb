{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural part of speech tagging. \n",
    "This is an optional assignment. Turn back whilst thou still can\n",
    "We're now going to solve the same problem of POS tagging with neural networks.\n",
    "From deep learning perspective, this is a task of predicting a sequence of outputs aligned to a sequence of inputs. There are several problems that match this formulation:\n",
    " * Part of Speech Tagging  - an auxuliary task for many NLP problems\n",
    " * Named Entity Recognition - for chat bots and web crawlers\n",
    " * Protein structure prediction - for bioinformatics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU') #используйте параметр ограничения роста памяти графического процессора\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Vera_Romantsova\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     C:\\Users\\Vera_Romantsova\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "data = nltk.corpus.brown.tagged_sents(tagset='universal')\n",
    "all_tags = ['#EOS#','#UNK#','ADV', 'NOUN', 'ADP', 'PRON', 'DET', '.', 'PRT', 'VERB', 'X', 'NUM', 'CONJ', 'ADJ']\n",
    "\n",
    "data = np.array([ [(word.lower(),tag) for word,tag in sentence] for sentence in data ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data,test_size=0.25,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>ADP</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>NOUN</td><td>VERB</td><td>ADV</td><td>VERB</td><td>ADP</td><td>DET</td><td>ADJ</td><td>NOUN</td><td>.</td></tr><td>implementation</td><td>of</td><td>georgia's</td><td>automobile</td><td>title</td><td>law</td><td>was</td><td>also</td><td>recommended</td><td>by</td><td>the</td><td>outgoing</td><td>jury</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>PRON</td><td>VERB</td><td>ADP</td><td>DET</td><td>NOUN</td><td>.</td><td>VERB</td><td>NOUN</td><td>PRT</td><td>VERB</td><td>.</td><td>DET</td><td>NOUN</td><td>.</td></tr><td>it</td><td>urged</td><td>that</td><td>the</td><td>city</td><td>``</td><td>take</td><td>steps</td><td>to</td><td>remedy</td><td>''</td><td>this</td><td>problem</td><td>.</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td>NOUN</td><td>VERB</td></tr><td>merger</td><td>proposed</td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "def draw(sentence):\n",
    "    words,tags = zip(*sentence)\n",
    "    display(HTML('<table><tr>{tags}</tr>{words}<tr></table>'.format(\n",
    "                words = '<td>{}</td>'.format('</td><td>'.join(words)),\n",
    "                tags = '<td>{}</td>'.format('</td><td>'.join(tags)))))\n",
    "    \n",
    "    \n",
    "draw(data[11])\n",
    "draw(data[10])\n",
    "draw(data[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('implementation', 'NOUN'),\n",
       " ('of', 'ADP'),\n",
       " (\"georgia's\", 'NOUN'),\n",
       " ('automobile', 'NOUN'),\n",
       " ('title', 'NOUN'),\n",
       " ('law', 'NOUN'),\n",
       " ('was', 'VERB'),\n",
       " ('also', 'ADV'),\n",
       " ('recommended', 'VERB'),\n",
       " ('by', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('outgoing', 'ADJ'),\n",
       " ('jury', 'NOUN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building vocabularies\n",
    "\n",
    "Just like before, we have to build a mapping from tokens to integer ids. This time around, our model operates on a word level,\n",
    "processing one word per RNN step. This means we'll have to deal with far larger vocavulary.\n",
    "Luckily for us, we only receive those words as input i.e. we don't have to predict them. This means we can have a large vocabulary \n",
    "for free by using word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage = 0.92876\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter()\n",
    "for sentence in data:\n",
    "    words,tags = zip(*sentence)\n",
    "    word_counts.update(words)\n",
    "\n",
    "all_words = ['#EOS#','#UNK#'] + list(list(zip(*word_counts.most_common(10000)))[0])\n",
    "\n",
    "#let's measure what fraction of data words are in the dictionary\n",
    "\n",
    "print(\"Coverage = %.5f\" % (float(sum(word_counts[w] for w in all_words)) / sum(word_counts.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "word_to_id = defaultdict(lambda:1, { word: i for i, word in enumerate(all_words) })\n",
    "tag_to_id = { tag: i for i, tag in enumerate(all_tags)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert words and tags into fix-size matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(lines, token_to_id, max_len=None, pad=0, dtype='int32', time_major=False):\n",
    "    \"\"\"Converts a list of names into rnn-digestable matrix with paddings added after the end\n",
    "    \"\"\"\n",
    "    max_len = max_len or max(map(len,lines))\n",
    "    matrix = np.empty([len(lines), max_len],dtype)\n",
    "    matrix.fill(pad)\n",
    "\n",
    "    for i in range(len(lines)):\n",
    "        line_ix = list(map(token_to_id.__getitem__,lines[i]))[:max_len]\n",
    "        matrix[i,:len(line_ix)] = line_ix\n",
    "\n",
    "    return matrix.T if time_major else matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word ids:\n",
      "[[   2 3057    5    2 2238 1334 4238 2454    3    6   19   26 1070   69\n",
      "     8 2088    6    3    1    3  266   65  342    2    1    3    2  315\n",
      "     1    9   87  216 3322   69 1558    4    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]\n",
      " [  45   12    8  511 8419    6   60 3246   39    2    1    1    3    2\n",
      "   845    1    3    1    3   10 9910    2    1 3470    9   43    1    1\n",
      "     3    6    2 1046  385   73 4562    3    9    2    1    1 3250    3\n",
      "    12   10    2  861 5240   12    8 8936  121    1    4]\n",
      " [  33   64   26   12  445    7 7346    9    8 3337    3    1 2811    3\n",
      "     2  463  572    2    1    1 1649   12    1    4    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0]]\n",
      "Tag ids:\n",
      "[[ 6  3  4  6  3  3  9  9  7 12  4  5  9  4  6  3 12  7  9  7  9  8  4  6\n",
      "   3  7  6 13  3  4  6  3  9  4  3  7  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]\n",
      " [ 5  9  6  9  3 12  6  3  7  6 13  3  7  6 13  3  7 13  7  5  9  6  3  3\n",
      "   4  6 13  3  7 12  6  3  6 13  3  7  4  6  3  9  3  7  9  4  6 13  3  9\n",
      "   6  3  2 13  7]\n",
      " [ 4  6  5  9 13  4  3  4  6 13  7 13  3  7  6  3  4  6 13  3  3  9  9  7\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "batch_words, batch_tags = zip(*[zip(*sentence) for sentence in data[-3:]])\n",
    "\n",
    "print(\"Word ids:\")\n",
    "print(to_matrix(batch_words, word_to_id))\n",
    "print(\"Tag ids:\")\n",
    "print(to_matrix(batch_tags, tag_to_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([('the', 'DET'), ('doors', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('d', 'NOUN'), ('train', 'NOUN'), ('slid', 'VERB'), ('shut', 'VERB'), (',', '.'), ('and', 'CONJ'), ('as', 'ADP'), ('i', 'PRON'), ('dropped', 'VERB'), ('into', 'ADP'), ('a', 'DET'), ('seat', 'NOUN'), ('and', 'CONJ'), (',', '.'), ('exhaling', 'VERB'), (',', '.'), ('looked', 'VERB'), ('up', 'PRT'), ('across', 'ADP'), ('the', 'DET'), ('aisle', 'NOUN'), (',', '.'), ('the', 'DET'), ('whole', 'ADJ'), ('aviary', 'NOUN'), ('in', 'ADP'), ('my', 'DET'), ('head', 'NOUN'), ('burst', 'VERB'), ('into', 'ADP'), ('song', 'NOUN'), ('.', '.')]),\n",
       "       list([('she', 'PRON'), ('was', 'VERB'), ('a', 'DET'), ('living', 'VERB'), ('doll', 'NOUN'), ('and', 'CONJ'), ('no', 'DET'), ('mistake', 'NOUN'), ('--', '.'), ('the', 'DET'), ('blue-black', 'ADJ'), ('bang', 'NOUN'), (',', '.'), ('the', 'DET'), ('wide', 'ADJ'), ('cheekbones', 'NOUN'), (',', '.'), ('olive-flushed', 'ADJ'), (',', '.'), ('that', 'PRON'), ('betrayed', 'VERB'), ('the', 'DET'), ('cherokee', 'NOUN'), ('strain', 'NOUN'), ('in', 'ADP'), ('her', 'DET'), ('midwestern', 'ADJ'), ('lineage', 'NOUN'), (',', '.'), ('and', 'CONJ'), ('the', 'DET'), ('mouth', 'NOUN'), ('whose', 'DET'), ('only', 'ADJ'), ('fault', 'NOUN'), (',', '.'), ('in', 'ADP'), ('the', 'DET'), (\"novelist's\", 'NOUN'), ('carping', 'VERB'), ('phrase', 'NOUN'), (',', '.'), ('was', 'VERB'), ('that', 'ADP'), ('the', 'DET'), ('lower', 'ADJ'), ('lip', 'NOUN'), ('was', 'VERB'), ('a', 'DET'), ('trifle', 'NOUN'), ('too', 'ADV'), ('voluptuous', 'ADJ'), ('.', '.')]),\n",
       "       list([('from', 'ADP'), ('what', 'DET'), ('i', 'PRON'), ('was', 'VERB'), ('able', 'ADJ'), ('to', 'ADP'), ('gauge', 'NOUN'), ('in', 'ADP'), ('a', 'DET'), ('swift', 'ADJ'), (',', '.'), ('greedy', 'ADJ'), ('glance', 'NOUN'), (',', '.'), ('the', 'DET'), ('figure', 'NOUN'), ('inside', 'ADP'), ('the', 'DET'), ('coral-colored', 'ADJ'), ('boucle', 'NOUN'), ('dress', 'NOUN'), ('was', 'VERB'), ('stupefying', 'VERB'), ('.', '.')])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[-3:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build model\n",
    "Unlike our previous lab, this time we'll focus on a high-level keras interface to recurrent neural networks. It is as simple as you can get with RNN, allbeit somewhat constraining for complex tasks like seq2seq.\n",
    "\n",
    "By default, all keras RNNs apply to a whole sequence of inputs and produce a sequence of hidden states (return_sequences=True or just the last hidden state (return_sequences=False). All the recurrence is happening under the hood.\n",
    "\n",
    "At the top of our model we need to apply a Dense layer to each time-step independently. As of now, by default keras.layers.Dense would apply once to all time-steps concatenated. We use keras.layers.TimeDistributed to modify Dense layer so that it would apply across both batch and time axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.layers as L\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.InputLayer([None],dtype='int32'))\n",
    "model.add(L.Embedding(len(all_words),50))\n",
    "model.add(L.SimpleRNN(64,return_sequences=True))\n",
    "\n",
    "#add top layer that predicts tag probabilities\n",
    "stepwise_dense = L.Dense(len(all_tags),activation='softmax')\n",
    "stepwise_dense = L.TimeDistributed(stepwise_dense)\n",
    "model.add(stepwise_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, None, 64)          7360      \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 14)          910       \n",
      "=================================================================\n",
      "Total params: 508,370\n",
      "Trainable params: 508,370\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training: in this case we don't want to prepare the whole training dataset in advance. The main cause is that the length of every batch depends on the maximum sentence length within the batch. This leaves us two options: use custom training code as in previous seminar or use generators.\n",
    "\n",
    "Keras models have a model.fit_generator method that accepts a python generator yielding one batch at a time. But first we need to implement such generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "BATCH_SIZE=32\n",
    "def generate_batches(sentences,batch_size=BATCH_SIZE,max_len=None,pad=0):\n",
    "    assert isinstance(sentences,np.ndarray),\"Make sure sentences is q numpy array\"\n",
    "    \n",
    "    while True:\n",
    "        indices = np.random.permutation(np.arange(len(sentences)))\n",
    "        for start in range(0,len(indices)-1,batch_size):\n",
    "            batch_indices = indices[start:start+batch_size]\n",
    "            batch_words,batch_tags = [],[]\n",
    "            for sent in sentences[batch_indices]:\n",
    "                words,tags = zip(*sent)\n",
    "                batch_words.append(words)\n",
    "                batch_tags.append(tags)\n",
    "\n",
    "            batch_words = to_matrix(batch_words,word_to_id,max_len,pad)\n",
    "            batch_tags = to_matrix(batch_tags,tag_to_id,max_len,pad)\n",
    "\n",
    "            batch_tags_1hot = to_categorical(batch_tags,len(all_tags)).reshape(batch_tags.shape+(-1,))\n",
    "            yield batch_words,batch_tags_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Callbacks: Another thing we need is to measure model performance. The tricky part is not to count accuracy after sentence ends (on padding) and making sure we count all the validation data exactly once.\n",
    "\n",
    "While it isn't impossible to persuade Keras to do all of that, we may as well write our own callback that does that. Keras callbacks allow you to write a custom code to be ran once every epoch or every minibatch. We'll define one via LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_test_accuracy(model):\n",
    "    test_words,test_tags = zip(*[zip(*sentence) for sentence in test_data])\n",
    "    test_words,test_tags = to_matrix(test_words,word_to_id),to_matrix(test_tags,tag_to_id)\n",
    "\n",
    "    #predict tag probabilities of shape [batch,time,n_tags]\n",
    "    predicted_tag_probabilities = model.predict(test_words,verbose=1)\n",
    "    predicted_tags = predicted_tag_probabilities.argmax(axis=-1)\n",
    "\n",
    "    #compute accurary excluding padding\n",
    "    numerator = np.sum(np.logical_and((predicted_tags == test_tags),(test_words != 0)))\n",
    "    denominator = np.sum(test_words != 0)\n",
    "    return float(numerator)/denominator\n",
    "\n",
    "\n",
    "class EvaluateAccuracy(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self,epoch,logs=None):\n",
    "        sys.stdout.flush()\n",
    "        print(\"\\nMeasuring validation accuracy...\")\n",
    "        acc = compute_test_accuracy(self.model)\n",
    "        print(\"\\nValidation accuracy: %.5f\\n\"%acc)\n",
    "        sys.stdout.flush()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vera_Romantsova\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 482s 357ms/step - loss: 0.5639\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 58s 129ms/step\n",
      "\n",
      "Validation accuracy: 0.93845\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 446s 332ms/step - loss: 0.0595\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 59s 131ms/step\n",
      "\n",
      "Validation accuracy: 0.94447\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 440s 327ms/step - loss: 0.0512\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 60s 134ms/step\n",
      "\n",
      "Validation accuracy: 0.94609\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 489s 364ms/step - loss: 0.0471\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 65s 145ms/step\n",
      "\n",
      "Validation accuracy: 0.94650\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 461s 343ms/step - loss: 0.0430\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 58s 131ms/step\n",
      "\n",
      "Validation accuracy: 0.94467\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21dc9aef1c0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure final accuracy on the whole test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 58s 129ms/step\n",
      "Final accuracy: 0.94467\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model)\n",
    "print(\"Final accuracy: %.5f\"%acc)\n",
    "assert acc>0.94, \"Keras has gone on a rampage again, please contact course staff.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going bidirectional\n",
    "Since we're analyzing a full sequence, it's legal for us to look into future data.\n",
    "\n",
    "A simple way to achieve that is to go both directions at once, making a bidirectional RNN.\n",
    "\n",
    "In Keras you can achieve that both manually (using two LSTMs and Concatenate) and by using keras.layers.Bidirectional.\n",
    "\n",
    "This one works just as TimeDistributed we saw before: you wrap it around a recurrent layer (SimpleRNN now and LSTM/GRU later) and it actually creates two layers under the hood.\n",
    "\n",
    "Your first task is to use such a layer our POS-tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a model that utilizes bidirectional SimpleRNN\n",
    "model2 = keras.models.Sequential()\n",
    "model2.add(L.InputLayer([None],dtype='int32'))\n",
    "model2.add(L.Embedding(len(all_words),50))\n",
    "model2.add(L.Bidirectional(L.SimpleRNN(64,return_sequences=True))) #Bidirectional\n",
    "model2.add(L.Dense(len(all_tags),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, None, 128)         14720     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, None, 14)          1806      \n",
      "=================================================================\n",
      "Total params: 516,626\n",
      "Trainable params: 516,626\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 843s 626ms/step - loss: 0.4589\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 108s 241ms/step\n",
      "\n",
      "Validation accuracy: 0.95658\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 892s 664ms/step - loss: 0.0427\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 112s 250ms/step\n",
      "\n",
      "Validation accuracy: 0.96131\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 923s 686ms/step - loss: 0.0341\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 110s 244ms/step\n",
      "\n",
      "Validation accuracy: 0.96270\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 1011s 752ms/step - loss: 0.0289\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 125s 279ms/step\n",
      "\n",
      "Validation accuracy: 0.96270\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 1014s 754ms/step - loss: 0.0242\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 131s 293ms/step\n",
      "\n",
      "Validation accuracy: 0.96223\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21dea5b4e50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model2.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 114s 256ms/step\n",
      "\n",
      "Final accuracy: 0.96223\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc = compute_test_accuracy(model2)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc)\n",
    "\n",
    "assert acc>0.96, \"Bidirectional RNNs are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task I: Structured loss functions (more bonus points)\n",
    "\n",
    "Since we're tagging the whole sequence at once, we might as well train our network to do so. Remember linear CRF from the lecture? You can also use it as a loss function for your RNN\n",
    "\n",
    "There's more than one way to do so, but we'd recommend starting with Conditional Random Fields\n",
    "You can plug CRF as a loss function and still train by backprop. There's even some neat tensorflow implementation for you.\n",
    "Alternatively, you can condition your model on previous tags (make it autoregressive) and perform beam search over that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimeDistributed + CRF\n",
    "model3 = keras.models.Sequential()\n",
    "model3.add(L.InputLayer([None],dtype='int32'))\n",
    "model3.add(L.Embedding(len(all_words),50))\n",
    "model3.add(L.Bidirectional(L.SimpleRNN(64,return_sequences=True)))\n",
    "model3.add(L.TimeDistributed(L.Dense(len(all_tags), activation=\"relu\")))\n",
    "crf_layer = CRF(len(all_tags))\n",
    "model3.add(crf_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         14720     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 14)          1806      \n",
      "_________________________________________________________________\n",
      "crf (CRF)                    (None, None, 14)          434       \n",
      "=================================================================\n",
      "Total params: 517,060\n",
      "Trainable params: 517,060\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(optimizer='adam', loss = crf_layer.loss_function, metrics=[crf_layer, 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "in user code:\n\n    C:\\Users\\Vera_Romantsova\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Vera_Romantsova\\anaconda3\\lib\\site-packages\\keras_contrib\\losses\\crf_losses.py:54 crf_loss  *\n        crf, idx = y_pred._keras_history[:2]\n\n    AttributeError: 'Tensor' object has no attribute '_keras_history'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-f02f0440f2b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m model3.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n\u001b[0m\u001b[0;32m      2\u001b[0m                     callbacks=[EvaluateAccuracy()], epochs=5,)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1845\u001b[0m                   \u001b[1;34m'will be removed in a future version. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[1;32m-> 1847\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1848\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    860\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2939\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m-> 2941\u001b[1;33m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[0;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3355\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m-> 3357\u001b[1;33m             return self._define_function_with_shape_relaxation(\n\u001b[0m\u001b[0;32m   3358\u001b[0m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3359\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[1;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[0;32m   3277\u001b[0m           expand_composites=True)\n\u001b[0;32m   3278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3279\u001b[1;33m     graph_function = self._create_graph_function(\n\u001b[0m\u001b[0;32m   3280\u001b[0m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3194\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3195\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3196\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3197\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3198\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    633\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 634\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    635\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    975\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 977\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    978\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: in user code:\n\n    C:\\Users\\Vera_Romantsova\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:805 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\Vera_Romantsova\\anaconda3\\lib\\site-packages\\keras_contrib\\losses\\crf_losses.py:54 crf_loss  *\n        crf, idx = y_pred._keras_history[:2]\n\n    AttributeError: 'Tensor' object has no attribute '_keras_history'\n"
     ]
    }
   ],
   "source": [
    "model3.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/issues/14464\n",
    "    \n",
    "у меня версия tf 2.4.1. - по ссылке проблема еще не решена - поэтому доделать не смогла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменим SimpleRNN на LSTM\n",
    "model4 = keras.models.Sequential()\n",
    "model4.add(L.InputLayer([None],dtype='int32'))\n",
    "model4.add(L.Embedding(len(all_words),50))\n",
    "model4.add(L.Bidirectional(L.LSTM(64,return_sequences=True))) #LSTM\n",
    "model4.add(L.Dense(len(all_tags),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 128)         58880     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, None, 14)          1806      \n",
      "=================================================================\n",
      "Total params: 560,786\n",
      "Trainable params: 560,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vera_Romantsova\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 60s 39ms/step - loss: 0.6043\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.95334\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 52s 39ms/step - loss: 0.0482\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.95977\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 52s 39ms/step - loss: 0.0376\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.96345\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 50s 37ms/step - loss: 0.0323\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.96454\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 51s 38ms/step - loss: 0.0282\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.96500\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x137be90e550>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model4.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Final accuracy: 0.96500\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc4 = compute_test_accuracy(model4)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc4)\n",
    "\n",
    "assert acc4>0.96223, \"last model are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Немного вырос accuracy, значительно выросла скорость обучения, так как добавили еще слои - попробуем увеличить кол-во эпох до 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1343/1343 [==============================] - 55s 38ms/step - loss: 0.6134\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.95370\n",
      "\n",
      "Epoch 2/10\n",
      "1343/1343 [==============================] - 51s 38ms/step - loss: 0.0473\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.95984\n",
      "\n",
      "Epoch 3/10\n",
      "1343/1343 [==============================] - 51s 38ms/step - loss: 0.0383\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.96355\n",
      "\n",
      "Epoch 4/10\n",
      "1343/1343 [==============================] - 50s 37ms/step - loss: 0.0325\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.96387\n",
      "\n",
      "Epoch 5/10\n",
      "1343/1343 [==============================] - 51s 38ms/step - loss: 0.0284\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.96521\n",
      "\n",
      "Epoch 6/10\n",
      "1343/1343 [==============================] - 52s 38ms/step - loss: 0.0242\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.96546\n",
      "\n",
      "Epoch 7/10\n",
      "1343/1343 [==============================] - 52s 39ms/step - loss: 0.0216\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 26ms/step\n",
      "\n",
      "Validation accuracy: 0.96482\n",
      "\n",
      "Epoch 8/10\n",
      "1343/1343 [==============================] - 53s 39ms/step - loss: 0.0191\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 27ms/step\n",
      "\n",
      "Validation accuracy: 0.96437\n",
      "\n",
      "Epoch 9/10\n",
      "1343/1343 [==============================] - 51s 38ms/step - loss: 0.0167\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.96346\n",
      "\n",
      "Epoch 10/10\n",
      "1343/1343 [==============================] - 52s 39ms/step - loss: 0.0144\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 26ms/step\n",
      "\n",
      "Validation accuracy: 0.96222\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1380b503760>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5 = keras.models.Sequential()\n",
    "model5.add(L.InputLayer([None],dtype='int32'))\n",
    "model5.add(L.Embedding(len(all_words),50))\n",
    "model5.add(L.Bidirectional(L.LSTM(64,return_sequences=True)))\n",
    "model5.add(L.Dense(len(all_tags),activation='softmax'))\n",
    "model5.compile('adam','categorical_crossentropy')\n",
    "model5.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=10,) # epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Final accuracy: 0.96222\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "last model are better than this!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-a73d69c41545>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\nFinal accuracy: %.5f\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[0macc5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0macc5\u001b[0m\u001b[1;33m>\u001b[0m\u001b[0macc4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"last model are better than this!\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Well done!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: last model are better than this!"
     ]
    }
   ],
   "source": [
    "acc5 = compute_test_accuracy(model5)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc5)\n",
    "\n",
    "assert acc5>acc4, \"last model are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# При увеличении числа эпох модель переобучается (loss снижается и val_accuracy тоже) - пока оставим 5 эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 54s 36ms/step - loss: 0.4918\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.95752\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 49s 36ms/step - loss: 0.0427\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 23ms/step\n",
      "\n",
      "Validation accuracy: 0.96256\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 47s 35ms/step - loss: 0.0351\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 24ms/step\n",
      "\n",
      "Validation accuracy: 0.96428\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 49s 37ms/step - loss: 0.0301\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 26ms/step\n",
      "\n",
      "Validation accuracy: 0.96543\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 49s 37ms/step - loss: 0.0265\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 24ms/step\n",
      "\n",
      "Validation accuracy: 0.96623\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1380ead0f70>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Попробуем использовать GRU вместо LSTM\n",
    "model6 = keras.models.Sequential()\n",
    "model6.add(L.InputLayer([None],dtype='int32'))\n",
    "model6.add(L.Embedding(len(all_words),50))\n",
    "model6.add(L.Bidirectional(L.GRU(64,return_sequences=True))) #GRU\n",
    "model6.add(L.Dense(len(all_tags),activation='softmax'))\n",
    "model6.compile('adam','categorical_crossentropy')\n",
    "model6.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Final accuracy: 0.96623\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc6 = compute_test_accuracy(model6)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc6)\n",
    "\n",
    "assert acc6>acc4, \"last model are better than this!\" # тк model5 завернули\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 54s 37ms/step - loss: 0.5212\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 12s 24ms/step\n",
      "\n",
      "Validation accuracy: 0.95638\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 50s 37ms/step - loss: 0.0457\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - ETA:  - 11s 24ms/step\n",
      "\n",
      "Validation accuracy: 0.96215\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 51s 38ms/step - loss: 0.0382\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 24ms/step\n",
      "\n",
      "Validation accuracy: 0.96447\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 49s 36ms/step - loss: 0.0333\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 24ms/step\n",
      "\n",
      "Validation accuracy: 0.96596\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 49s 36ms/step - loss: 0.0305\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 11s 25ms/step\n",
      "\n",
      "Validation accuracy: 0.96642\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13819b8cf70>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Немного вырос accuracy по сравнению с LSTM, оставим\n",
    "# Добавим Dropout\n",
    "model7 = keras.models.Sequential()\n",
    "model7.add(L.InputLayer([None],dtype='int32'))\n",
    "model7.add(L.Embedding(len(all_words),50))\n",
    "model7.add(L.SpatialDropout1D(0.2))\n",
    "model7.add(L.Bidirectional(L.GRU(64,return_sequences=True)))\n",
    "model7.add(L.Dense(len(all_tags),activation='softmax'))\n",
    "model7.compile('adam','categorical_crossentropy')\n",
    "model7.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 11s 24ms/step\n",
      "\n",
      "Final accuracy: 0.96642\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc7 = compute_test_accuracy(model7)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc7)\n",
    "\n",
    "assert acc7>acc6, \"last model are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Немного вырос accuracy, оставим\n",
    "# Добавим Conv1D\n",
    "model9 = keras.models.Sequential()\n",
    "model9.add(L.InputLayer([None],dtype='int32'))\n",
    "model9.add(L.Embedding(len(all_words),50))\n",
    "model9.add(L.SpatialDropout1D(0.2))\n",
    "model9.add(L.Conv1D(64, kernel_size = 1)) #Conv1D\n",
    "model9.add(L.Bidirectional(L.GRU(64,return_sequences=True)))\n",
    "model9.add(L.Dropout(0.2))\n",
    "model9.add(L.Bidirectional(L.GRU(64,return_sequences=True)))\n",
    "model9.add(L.Dense(len(all_tags),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_7 (Spatial (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, None, 64)          3264      \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, None, 128)         49920     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, None, 128)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_16 (Bidirectio (None, None, 128)         74496     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, None, 14)          1806      \n",
      "=================================================================\n",
      "Total params: 629,586\n",
      "Trainable params: 629,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model9.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1343/1343 [==============================] - 83s 54ms/step - loss: 0.4072\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 21s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.95803\n",
      "\n",
      "Epoch 2/5\n",
      "1343/1343 [==============================] - 73s 54ms/step - loss: 0.0471\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96255\n",
      "\n",
      "Epoch 3/5\n",
      "1343/1343 [==============================] - 74s 55ms/step - loss: 0.0401\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 41ms/step\n",
      "\n",
      "Validation accuracy: 0.96496\n",
      "\n",
      "Epoch 4/5\n",
      "1343/1343 [==============================] - 76s 56ms/step - loss: 0.0362\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96642\n",
      "\n",
      "Epoch 5/5\n",
      "1343/1343 [==============================] - 78s 58ms/step - loss: 0.0337\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 41ms/step\n",
      "\n",
      "Validation accuracy: 0.96712\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c6ad686760>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model9.compile('adam','categorical_crossentropy')\n",
    "model9.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=5,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 19s 43ms/step\n",
      "\n",
      "Final accuracy: 0.96712\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc9 = compute_test_accuracy(model9)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc9)\n",
    "\n",
    "assert acc9>0.96642, \"last model are better than this!\" #перезапускала ноутбук, чтобы все не прогонять, вставила accuracy с прошлой модели model8\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model10 = keras.models.Sequential()\n",
    "model10.add(L.InputLayer([None],dtype='int32'))\n",
    "model10.add(L.Embedding(len(all_words),50))\n",
    "model10.add(L.SpatialDropout1D(0.2))\n",
    "model10.add(L.Conv1D(64, kernel_size = 1)) \n",
    "model10.add(L.Bidirectional(L.GRU(64,return_sequences=True, dropout = 0.2))) #dropout\n",
    "model10.add(L.Bidirectional(L.GRU(64,return_sequences=True, dropout = 0.1))) #dropout\n",
    "model10.add(L.Dense(len(all_tags),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_13 (Spatia (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, None, 64)          3264      \n",
      "_________________________________________________________________\n",
      "bidirectional_27 (Bidirectio (None, None, 128)         49920     \n",
      "_________________________________________________________________\n",
      "bidirectional_28 (Bidirectio (None, None, 128)         74496     \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, None, 14)          1806      \n",
      "=================================================================\n",
      "Total params: 629,586\n",
      "Trainable params: 629,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model10.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1343/1343 [==============================] - 85s 56ms/step - loss: 0.3893\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 21s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.95828\n",
      "\n",
      "Epoch 2/10\n",
      "1343/1343 [==============================] - 77s 58ms/step - loss: 0.0462\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96340\n",
      "\n",
      "Epoch 3/10\n",
      "1343/1343 [==============================] - 81s 60ms/step - loss: 0.0392\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 43ms/step\n",
      "\n",
      "Validation accuracy: 0.96518\n",
      "\n",
      "Epoch 4/10\n",
      "1343/1343 [==============================] - 79s 58ms/step - loss: 0.0355\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96653\n",
      "\n",
      "Epoch 5/10\n",
      "1343/1343 [==============================] - 77s 58ms/step - loss: 0.0333\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96729\n",
      "\n",
      "Epoch 6/10\n",
      "1343/1343 [==============================] - 78s 58ms/step - loss: 0.0307\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96756\n",
      "\n",
      "Epoch 7/10\n",
      "1343/1343 [==============================] - 78s 58ms/step - loss: 0.0299\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96828\n",
      "\n",
      "Epoch 8/10\n",
      "1343/1343 [==============================] - 78s 58ms/step - loss: 0.0279\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96808\n",
      "\n",
      "Epoch 9/10\n",
      "1343/1343 [==============================] - 78s 58ms/step - loss: 0.0268\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96806\n",
      "\n",
      "Epoch 10/10\n",
      "1343/1343 [==============================] - 78s 58ms/step - loss: 0.0257\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 19s 42ms/step\n",
      "\n",
      "Validation accuracy: 0.96775\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c6b8885d00>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model10.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model10.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=10,) #epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 19s 43ms/step\n",
      "\n",
      "Final accuracy: 0.96775\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc10 = compute_test_accuracy(model10)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc10)\n",
    "\n",
    "assert acc10>acc9, \"last model are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model11 = keras.models.Sequential()\n",
    "model11.add(L.InputLayer([None],dtype='int32'))\n",
    "model11.add(L.Embedding(len(all_words),50))\n",
    "model11.add(L.SpatialDropout1D(0.2))\n",
    "model11.add(L.Conv1D(64, kernel_size = 1)) \n",
    "model11.add(L.Bidirectional(L.GRU(128,return_sequences=True, dropout = 0.2))) #128\n",
    "model11.add(L.Bidirectional(L.GRU(64,return_sequences=True, dropout = 0.1))) #dropout\n",
    "model11.add(L.Dense(len(all_tags),activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, None, 50)          500100    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_14 (Spatia (None, None, 50)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, None, 64)          3264      \n",
      "_________________________________________________________________\n",
      "bidirectional_29 (Bidirectio (None, None, 256)         148992    \n",
      "_________________________________________________________________\n",
      "bidirectional_30 (Bidirectio (None, None, 128)         123648    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, None, 14)          1806      \n",
      "=================================================================\n",
      "Total params: 777,810\n",
      "Trainable params: 777,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model11.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1343/1343 [==============================] - 92s 62ms/step - loss: 0.3641 \n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 25s 50ms/step\n",
      "\n",
      "Validation accuracy: 0.95928\n",
      "\n",
      "Epoch 2/10\n",
      "1343/1343 [==============================] - 89s 66ms/step - loss: 0.0444\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 23s 50ms/step\n",
      "\n",
      "Validation accuracy: 0.96347\n",
      "\n",
      "Epoch 3/10\n",
      "1343/1343 [==============================] - 85s 64ms/step - loss: 0.0384\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 22s 49ms/step\n",
      "\n",
      "Validation accuracy: 0.96596\n",
      "\n",
      "Epoch 4/10\n",
      "1343/1343 [==============================] - 86s 64ms/step - loss: 0.0345\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 22s 49ms/step\n",
      "\n",
      "Validation accuracy: 0.96669\n",
      "\n",
      "Epoch 5/10\n",
      "1343/1343 [==============================] - 86s 64ms/step - loss: 0.0322\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 22s 50ms/step\n",
      "\n",
      "Validation accuracy: 0.96756\n",
      "\n",
      "Epoch 6/10\n",
      "1343/1343 [==============================] - 84s 63ms/step - loss: 0.0306\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 22s 49ms/step\n",
      "\n",
      "Validation accuracy: 0.96787\n",
      "\n",
      "Epoch 7/10\n",
      "1343/1343 [==============================] - 87s 65ms/step - loss: 0.0286\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 22s 50ms/step\n",
      "\n",
      "Validation accuracy: 0.96817\n",
      "\n",
      "Epoch 8/10\n",
      "1343/1343 [==============================] - 86s 64ms/step - loss: 0.0273\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 24s 54ms/step\n",
      "\n",
      "Validation accuracy: 0.96822\n",
      "\n",
      "Epoch 9/10\n",
      "1343/1343 [==============================] - 88s 66ms/step - loss: 0.0265\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 24s 52ms/step\n",
      "\n",
      "Validation accuracy: 0.96844\n",
      "\n",
      "Epoch 10/10\n",
      "1343/1343 [==============================] - 86s 64ms/step - loss: 0.0246\n",
      "\n",
      "Measuring validation accuracy...\n",
      "448/448 [==============================] - 23s 51ms/step\n",
      "\n",
      "Validation accuracy: 0.96838\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c6ef2b1520>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model11.compile('adam','categorical_crossentropy')\n",
    "\n",
    "model11.fit_generator(generate_batches(train_data),len(train_data)/BATCH_SIZE,\n",
    "                    callbacks=[EvaluateAccuracy()], epochs=10,) #epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "448/448 [==============================] - 23s 52ms/step\n",
      "\n",
      "Final accuracy: 0.96838\n",
      "Well done!\n"
     ]
    }
   ],
   "source": [
    "acc11 = compute_test_accuracy(model11)\n",
    "print(\"\\nFinal accuracy: %.5f\"%acc11)\n",
    "\n",
    "assert acc11>acc10, \"last model are better than this!\"\n",
    "print(\"Well done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some tips\n",
    "Here there are a few more tips on how to improve training that are a bit trickier to impliment. We strongly suggest that you try them after you've got a good initial model.\n",
    "\n",
    "* Use pre-trained embeddings: you can use pre-trained weights from there to kickstart your Embedding layer.\n",
    "* Embedding layer has a matrix W (layer.W) which contains word embeddings for each word in the dictionary. You can just overwrite them with tf.assign.\n",
    "* When using pre-trained embeddings, pay attention to the fact that model's dictionary is different from your own.\n",
    "* You may want to switch trainable=False for embedding layer in first few epochs as in regular fine-tuning.\n",
    "\n",
    "* Go beyond SimpleRNN: there's keras.layers.LSTM and keras.layers.GRU\n",
    "* If you want to use a custom recurrent Cell, read this\n",
    "* You can also use 1D Convolutions (keras.layers.Conv1D). They are often as good as recurrent layers but with less overfitting.\n",
    "\n",
    "* Stack more layers: if there is a common motif to this course it's about stacking layers\n",
    "* You can just add recurrent and 1dconv layers on top of one another and keras will understand it\n",
    "* Just remember that bigger networks may need more epochs to train\n",
    "\n",
    "* Regularization: you can apply dropouts as usual but also in an RNN-specific way\n",
    "* keras.layers.Dropout works inbetween RNN layers\n",
    "* Recurrent layers also have recurrent_dropout parameter\n",
    "\n",
    "* Gradient clipping: If your training isn't as stable as you'd like, set clipnorm in your optimizer.\n",
    "* Which is to say, it's a good idea to watch over your loss curve at each minibatch. Try tensorboard callback or something similar.\n",
    "\n",
    "* Word Dropout: tl;dr randomly replace words with UNK during training.\n",
    "* This can also simulate increased amount of unknown words in the test set\n",
    "\n",
    "* Larger vocabulary: You can obtain greater performance by expanding your model's input dictionary from 5000 to up to every single word!\n",
    "* Just make sure your model doesn't overfit due to so many parameters.\n",
    "* Combined with regularizers or pre-trained word-vectors this could be really good cuz right now our model is blind to >5% of words.\n",
    "\n",
    "* More efficient batching: right now TF spends a lot of time iterating over \"0\"s\n",
    "* This happens because batch is always padded to the length of a longest sentence\n",
    "* You can speed things up by pre-generating batches of similar lengths and feeding it with randomly chosen pre-generated batch.\n",
    "* This technically breaks the i.i.d. assumption, but it works unless you come up with some insane rnn architectures.\n",
    "\n",
    "* The most important advice: don't cram in everything at once!\n",
    "* If you stuff in a lot of modiffications, some of them almost inevitably gonna be detrimental and you'll never know which of them are.\n",
    "* Try to instead go in small iterations and record experiment results to guide further search.\n",
    "Good hunting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
